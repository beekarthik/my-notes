% !TEX root = ./main.tex

\section{Inner Product Spaces}

\subsection{Inner Products and Norms}

To motivate the definition of the inner product, consider $\R^n$.

Some vector $x = (x_1, x_2)$ has length $\sqrt{x_1^2 + x_2^2}$.

For $x = (x_1, \dots, x_n) \in \R^n$, define the norm of $x$ by
$\norm{x} = \sqrt{x_1^2 + \dots + x_n^2}$.

\begin{definition} [Dot Product]
    For $x, y \in \R^n$, the \textbf{dot product} of $x$ and $y$,
    denoted $x \cdot y$, is defined by:
    \[ x \cdot y = x_1 y_1 + \dots + x_n y_n \]
    where $x = (x_1, \dots, x_n)$ and $y = (y_1, \dots, y_n)$.
\end{definition}

The dot product has the following properties:
\begin{itemize}
    \item $x \cdot x = \abs{x}^2$ for all $x \in \R^n$.
    \item $x \cdot x \geq 0$ for all $x \in \R^n$, with equality iff $x = 0$
    \item for $y \in \R^n$ fixed, the map from $\R^n$ to $\R$ that sends $x \in \R^n$ to $x \cdot y$ is linear
    \item $x \cdot y = y \cdot x$ for all $x, y \in \R^n$
\end{itemize}

It gets a little bit more complicated for complex numbers.

Recall that if $\lambda = a + bi$ where $a, b \in \R$, then
\begin{itemize}
    \item $\abs{\lambda} = \sqrt{a^2 + b^2}$
    \item $\abs{\lambda}^2 = \lambda \overline{\lambda}$
\end{itemize}

For $z = (z_1, \dots, z_n) \in \C^n$, define the norm of $z$ by
\[ \norm{z} = sqrt{\abs{z_1}^2 + \dots + \abs{z_n}^2} \]

This suggests that the inner product of $w = (w_1, \dots, w_n) \in \C^n$ with $z$
should equal
\[ w_1 \overline{z_1} + \dots + w_n \overline{z_n} \]

\begin{definition} [Inner Product]
    An \textbf{inner product} on $V$ is a function that takes each ordered pair
    $(u, v)$ of elements in $V$ to a number $\inner{u, v} \in \F$ that has the folowing properties
    \begin{itemize}
        \item $\inner{v, v} \geq 0$ for all $v \in V$
        \item $\inner{v, v} = 0$ iff $v = 0$
        \item $\inner{u + v, w} = \inner{u, w} + \inner{v, w}$ for all $u, v, w \in V$
        \item $\inner{\lambda u, v} = \lambda \inner{u, v}$ for all $\lambda \in \F$ and all $u, v \in V$
        \item $\inner{u, v} = \overline{\inner{v, u}}$ for all $u, v \in V$
    \end{itemize}
\end{definition}

\begin{example} Examples of inner products:
    \begin{itemize}
        \item The \textbf{Euclidean inner product} on $\F^n$ is defined by
        \[ \inner{(w_1, \dots, w_n)(z_1, \dots, z_n)} = w_1 \overline{z_1} + \dots + w_n \overline{z_n} \]
        \item If $c_1, \dots, c_n$ are positive numbers, then an inner product can be defined on $\F^n$ by
        \[ \inner{(w_1, \dots, w_n), (z_1, \dots, z_n)} = c_1 w_1 \overline{z_1} + \dots + c_n w_n \overline{z_n} \]
        \item An inner product can be defined on the vector space of continuous real-valued functions on the interval $[-1, 1]$ by
        \[ \inner{f, g} = \int_{-1}^1 f(x) g(x) \dd{x} \]
    \end{itemize}
\end{example}

\begin{definition} [Inner Product Space]
    An \textbf{inner product space} is a vector space $V$ along with an inner product on $V$.
\end{definition}

\begin{theorem}
    Properties of an inner product:
    \begin{itemize}
        \item For each fixed $u \in V$, the function that takes $v$ to $\inner{u, v}$ is a linear map from $V$ to $\F$
        \item $\inner{0, u} = 0$ for every $u \in V$.
        \item $\inner{u, 0} = 0$ for every $u \in V$.
        \item $\inner{u, v + w} = \inner{u, v} + \inner{u, w}$ for all $u, v, w \in V$
        \item $\inner{u, \lambda v} = \overline{\lambda} \inner{u, v}$ for all $\lambda \in \F$ and $u, v \in V$
    \end{itemize}
\end{theorem}

\begin{definition}
    For $v \in V$, the \textbf{norm} of $v$, denoted $\norm{v}$, is defined by
    \[ \norm{v} = \sqrt{\inner{v, v}} \]
\end{definition}

In $\F^n$, always assume the standard Euclidean inner product, to make the norm the standard
Euclidean norm.

Some properties of the norm:
\begin{itemize}
    \item Norm is positive-definite.
    \item $\norm{\lambda v} =  \abs{\lambda} \norm{v}$ for all $\lambda \in \F$
\end{itemize}

\begin{definition} [Orthogonal]
    Two vectors $u, v \in V$ are called \textbf{orthogonal} if $\inner{u, v} = 0$.
\end{definition}

\begin{theorem}
    0 is orthogonal to every vector in $V$.

    0 is the only vector in $V$ that is orthogonal to itself.
\end{theorem}

\begin{theorem} [Pythagorean Theorem]
    Suppose $u$ and $v$ are orthogonal vectors in $V$. Then
    \[ \norm{u + v}^2 = \norm{u}^2 + \norm{v}^2 \]

    \begin{proof*}
        \begin{align*}
            \norm{u + v}^2 &= \inner{u + v, u + v} \\
            &= \inner{u, u} + \inner{u, v} + \inner{v, u} + \inner{v, v} \\
            &= \norm{u}^2 + \norm{v}^2
        \end{align*} \qed
    \end{proof*}
\end{theorem}

\begin{theorem} [Cauchy-Schwarz Inequality]
    Suppose $u, v \in V$. Then
    \[ \abs{\inner{u, v}} \leq \norm{u} \norm{v} \]
    Equality holds if and only if $u$ and $v$ and linearly dependent.
\end{theorem}

\begin{example}
    Examples of Cauchy-Schwarz:
    \begin{itemize}
        \item If $x_1, \dots, x_n, y_1, \dots, y_n \in \R$, then
        \[ \abs{x_1 y_1 + \dots + x_n y_n}^2 \leq (x_1^2 + \dots + x_n^2)(y_1^2 + \dots + y_n^2) \]
    \end{itemize}
\end{example}

\begin{theorem} [Triangle Inequality]
    Suppose $u, v \in V$. Then
    \[ \norm{u + v} \leq \norm{u} + \norm{v} \]
    Equality holds if and only if the $u, v$ are linearly dependent.
\end{theorem}

\begin{theorem} [Parallelogram Equality]
    Suppose $u, v \in V$. Then
    \[ \norm{u + v}^2 + \norm{u - v}^2 = 2(\norm{u}) \]
\end{theorem}

A generally good way to do proofs of these identities is to write $\norm{x}^2 = \inner{x , x}$ and then
use the algebraic properties of the inner product.

\subsection{Orthonormal Bases}

Another section lost to the winds of time. (This is what Github Copilot recommends.)

\subsection{Orthogonal Complements and Minimization Problems}

\begin{definition} [Orthogonal Complement, $U^\perp$]
    If $U$ is a subset of $V$, then the \textbf{orthogonal complement} of $U$, denoted $U^\perp$, is the set of all
    vectors in $V$ that are orthogonal to every vector in $U$.
    \[ U^\perp = \{v \in V : \innerproduct{v}{u} = 0 \text{ for every } u \in U\} \]
\end{definition}

\begin{theorem}
    Some properties of $U^\perp$:
    \begin{itemize}
        \item If $U$ is a subset of $V$, then $U^\perp$ is a subspace of $V$.
        \item $\{0\}^\perp = \{0\}$.
        \item $V^\perp = \{0\}$.
        \item If $U$ is a subset of $V$, then $U \cap U^\perp \subseteq \{0\}$. ($U$ might not contain $0$.)
        \item If $U$ and $W$ are subsets of $V$ and $U \subseteq W$, then $W^\perp \in U^\perp$.
    \end{itemize}
\end{theorem}

\begin{theorem} [Direct Sum of a Subspace and its Orthogonal Complement]
    Suppose $U$ is a finite-dimensional subspace of $V$. Then
    \[ V = U \oplus U^\perp \]

    \begin{proof*}
        Suppose $v \in V$. Let $e_1, \dots, e_m$ be an orthonormal basis of $U$. Let
        \[ u = \innerproduct{v}{e_1}e_1 + \dots + \innerproduct{v}{e_m}e_m \]
        Then $\innerproduct{v - u}{e_j} = \innerproduct{v}{e_j} - \innerproduct{u}{e_j} = 0$ for all $j$. Thus $v - u \in U^\perp$.
        Now
        \[ v = u + (v - u) \]
        showing that $v \in U + U^\perp$. Thus $V = U \oplus U^\perp$. \qed
    \end{proof*}
\end{theorem}

\begin{theorem} [Dimension of $U^\perp$]
    Suppose $V$ is finite-dimensional and $U$ is a subspace of $V$. Then
    \[ \dim U^\perp = \dim V - \dim U \]
\end{theorem}

\begin{theorem}
    Suppose $U$ is a finite-dimensional subspace of $V$. Then
    \[ U = (U^\perp)^\perp \]
\end{theorem}

\begin{definition}
    Suppose $U$ is a finite-dimensional subspace of $V$. The \textbf{orthogonal projection} of $V$ onto $U$
    is the operator $P_U \in \Lin{V}$ defined as follows:

    For $v \in V$, write $v = u + w$, where $u \in U$ and $w \in U^\perp$. Then $P_U(v) = u$.

    If $e_1, \dots, e_m$ are orthonormal basis of $U$, then
    \[ P_U(v) = \innerproduct{v}{e_1}e_1 + \dots + \innerproduct{v}{e_m}e_m \]
\end{definition}

\begin{theorem}
    Suppose $U$ is a finite-dimensional subspace of $V$. Then
    \begin{itemize}
        \item $P_U \in \Lin{V}$.
        \item $P_U u = u$ for every $u \in U$.
        \item $P_U w = 0$ for every $w \in U^\perp$.
        \item $\range P_U = U$
        \item $\Null P_U = U^\perp$
        \item $v - P_Uv \in U^\perp$ for every $v \in V$
        \item $P_U^2 = P_U$
    \end{itemize}
\end{theorem}

Here is a common minimization problem.
Given a subspace $U$ of $V$ and a point $v \in V$, find the point $u \in U$ that minimizes the distance
$|| u - v ||$.

\begin{theorem}
    Suppose $U$ is a finite-dimensional subspace of $V$ and $v \in V$. Then
    \[ \norm{v - P_Uv} \leq \norm{v - u} \]
    for all $u \in U$.
\end{theorem}

\begin{example}
    Here is a minimization problem we can now solve. Find $u \in \Polys{5}{\R}$ that approximates
    $\sin x$ as well as possible on the interval $[-\pi, \pi]$, in the sense that
    \[ \int_{-\pi}^{\pi} \abs{\sin x - u(x)}^2 \dd{x} \]
    is minimized.

    Here $V = C_{\R} [-\pi, \pi]$ with inner product
    $\innerproduct{f}{g} = \int_{-\pi}^{\pi} f(x) g(x) \, \dd{x}$.
    Also $v \in V$ is given by $v(x) = \sin x$ and $U = \Polys{5}{\R}$.

    Thus:
    \begin{align*}
        u &= P_Uv \\
        &= \sum_{j=1}^6 \innerproduct{v, e_j}e_j \\
    \end{align*}
    where $e_1, \dots, e_6$ is an orthonormal basis of $U$.
    Find $e_1, \dots, e_6$ by applying Gram-Schmidt procedure to the basis
    $1, x, x^2, x^3, x^4, x^5$ of $U$.

    Then use the last equation to get
    \[ u = \alpha x - \beta x^3 + \gamma x^5 \]
    where the coefficients are way too complicated to write down. Note that this is NOT
    the Taylor series. However, the coefficients are fairly close.
\end{example}
\endinput