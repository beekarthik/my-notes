% !TEX root = ./main.tex

\section{Vector Spaces}

\subsection{$\R^n$ and $\C^n$}

\begin{definition}[$R$]
    $\R$ denotes the field of real numbers.
\end{definition}
    
Some nonconstant polynomials with real coefficients have no real zeroes.
Example: the equation:
\[ x^2 + 1 = 0\]
has no real solutions. Thus, we invent a solution
called $i$, such that $i^2 = -1$.

\begin{definition}[Complex Numbers] $\empty$
    \begin{itemize}
        \item A complex number is an ordered pair $(a, b)$, where $a, b \in \R$, but we will write this as $a+bi$.
        \item The set of all complex numbers is denoted by $\C$:
            \[ \C = \{a+bi : a, b \in \R\} \]
        \item Addition and multiplication on $\C$ are defined as follows
        \[ (a+bi) + (c+di) = (a+c)+(b+d)i \]
        \[ (a+bi)(c+di) = (ac-bd) + (ad+bc)i \]
    \end{itemize}
\end{definition}

\begin{note}
    If $a \in \R$, we identify $a+0i$ with the real number $a$.
    Thus we think of $\R$ as a subset of $\C$. We also usually write
    $0+bi$ as just $bi$, and we usually write $0+1i$ as just $i$. From
    the definition of multiplication above, we have that $i^2 = -1$.
\end{note}

\begin{note}[Properties of Complex Arithmetic]
    $\forall \alpha, \beta \in \C$
    \begin{itemize}
        \item Commutativity
        \[ \alpha + \beta = \beta + \alpha \text{ and } \alpha \beta = \beta \alpha\]
        \item Associativity
        \[ (\alpha + \beta) + \lambda = \alpha + (\beta + \lambda) \text{ and } (\alpha\beta)\lambda = \alpha(\beta \lambda) \]
        \item Identities
        \[ \lambda + 0 = \lambda \text{ and } \lambda 1 = \lambda \]
        \item Additive Inverse
        \begin{center}
           For every $\alpha \in \C$ there exists a unique $\beta \in \C$ such that $\alpha + \beta = 0$ 
        \end{center}
        \item Multiplicative Inverse
        \begin{center}
            For every  $\alpha \in \C \setminus \{0\}$, there exists a unique $\beta \in \C$ such that $\alpha \beta = 1$ 
        \end{center}
        \item Distributivity
        \[ \lambda(\alpha + \beta) = \lambda \alpha + \lambda \beta \]
    \end{itemize}
\end{note}

\begin{definition}[$\F$]
    $\mathbb{F}$ denotes either $\R$ or $\C$
\end{definition}
Elements of $\mathbb{F}$ are sometimes called scalars.
We call it $\mathbb{F}$ because those are both fields.

Now we discuss the idea of a "list." To understand the idea, here some examples of
simple sets we have already seen in other mathematics:
\begin{itemize}
    \item The set $\R^2$, which you can think of as a plane, is the set of all ordered
    pairs of real numbers:
    \[ \R^2 = \{(x, y) : x, y \in \R\} \]
    \item The set $\R^3$, which you can think of as ordinary space, is the set of all
    ordered triples of real numbers:
    \[ \R^3 = \{(x, y, z) : x, y, z \in \R\} \]
\end{itemize}

\begin{definition}[List]
    A \textit{list} of \textit{length $n$} is an ordered collection of $n$ numbers
    separated by commas and surrounded by parenthesis.
    
    \[ \text{i.e. }(x_1, \ldots, x_n) \]
    
    Two lists are equal if an only if they have the same length and the same elements. 
\end{definition}

Here are some examples of lists from sets we are familiar with:
\begin{enumerate}
    \item $(7, 3)$ is a list of length 2. Thus $(7, 3) \in \R^2$.
    \item $(5, 9, -2)$ is a list of length 3. Thus $(5, 9, -2) \in \R^3$
\end{enumerate}

\begin{definition}[$\F^n$]
    $\F^n$ is the set of all lists of length $n$ of elements of $\F$:
    \[ \F^n = \{(x_1, \ldots, x_n) : x_j \in \F \text{ for } j = 1, \ldots, n\} \]
\end{definition}

Elements of $\F^n$ are often called \textit{points} or \textit{vectors}. 

It does not matter if these sets have geometric sense. We can manipulate them algebraically.
This is where the name linear \underline{algebra} comes from.

\begin{definition}[Addition in $\F^n$]
    Addition in $\F^n$ is defined by adding the corresponding coordinates:
    \[ (x_1, \ldots, x_n) + (y_1, \ldots, y_n) = (x_1+y_1, \ldots, x_n+y_n) \]
\end{definition}

\begin{definition}[Scalar Multiplication in $\F^n$]
    The product of a number $\lambda \in \F$ and a vector $\F^n$ is defined by
    multiplying each coordinate of the vector by $\lambda$:
    \[\lambda (x_1, \ldots, x_n) = (\lambda x_1, \ldots, \lambda x_n) \] 
\end{definition}

Single letters can denote elements of $\F^n$ efficiently. You can say $x + y = z$ instead of saying e.g.
\[ (x_1, \ldots, x_n) + (y_1, \ldots, y_n) = (z_1, \ldots, z_n) \]

\begin{definition}[0 list]
    Let 0 denote the list of length $n$ whose coordinates are all 0:
    \[ 0 = (0, \ldots, 0) \]
\end{definition}

It should always be clear from context which 0 you're talking about. For example:
we have the following:
\begin{theorem*}
    If $x \in \F^n$, then $0x = 0$.
\end{theorem*}

The 0 on the LHS is a scalar in $\F$. The 0 on the RHS is a vector in $\F^n$.

\subsection{Definition of a Vector Space}

The motivation for the definition of a vectors space comes from the
properties of addition and scalar multiplication in $\F^n$:
\begin{itemize}
    \item Addition is commutative, associative, and has an identity.
    \item Every element has an additive inverse.
    \item Scalar multiplication is associative.
    \item Scalar multiplication by 1 acts as expected.
    \item Addition and scalar multiplication are connected by distributive properties.
\end{itemize}

First, let us define what addition/scalar multiplication is.
\begin{definition}[Addition, Scalar Multiplication] 
    $\empty$ \\
    \begin{itemize}
        \item An \textit{addition} on a set $V$ is a function that assigns an element $u + w \in V$ to each pair of elements $u, w \in V$
        \item A \textit{scalar multiplication} on a set $V$ is a function that assigns an element $\lambda u \in V$ to each $\lambda \in \F$ and each $u \in V$
    \end{itemize}
\end{definition}

\begin{example}
    Suppose $V$ is the set of real valued functions on the interval $[0, 1]$.
    
    For $f, g \in V$ and $\lambda \in \R$, define $f + g$ and $\lambda f$ by:
    \[ (f+g)(x) = f(x) + g(x) \]
    and
    \[ (\lambda f)(x) = \lambda f(x) \]
    Thus $f + g \in V$ and $\lambda f \in V$.
\end{example}

Now, we can define a vector space $V$. These are based off the
properties of $\F^n$:

\begin{definition}[Vector Space]
    A \textit{vector space} is a set $V$ along with an addition on $V$ and a
    scalar multiplication on $V$ such that the following properties hold:

    \begin{itemize}
        \item $u + w = w + u$ for all $u, w \in V$
        \item $(u + v) + w = u + (v + w)$ and $(ab)u = a(bu)$ for all $u, v, w \in V$ and all $a, b \in \F$
        \item There exists $0 \in V$ such that $u + 0 = u$ for all $u \in V$
        \item For every $u \in V$, there exists $w \in V$ such that $u + w = 0$
        \item 1$u = u$ for all $u \in V$
        \item $a (u + w) = au + aw$ and $(a+b)u = au + bu$ for all $a, b \in \F$ and all $u, w \in V$
    \end{itemize}
\end{definition}

\begin{example} Vector Spaces:
    \begin{itemize}
        \item $\F^n$ with the usual operations of addition and scalar multiplications is a vector space.
        \item $\F^\infty$ is defined to be the set of all sequences of elements of $\F$:
        \[ \{(x_1, x_2, \ldots) : x_j \in \F \text{for } j = 1, 2, \ldots \} \]
        Addition and scalar multiplication are also defined coordinate-wise. This is also a vector space.
        \item More generally, if $S$ is a set, let $\F^S$ denote the set of functions from $S$ to $\F$.
        For $f, g \in \F^S$, the sum $f + g \in \F^S$ is the function defined by:
        \[ (f + g)(x)  = f(x) + g(x)\]
        for all $x \in S$.
        For $\lambda \in \F$ and $f \in \F^S$, the product $\lambda f \in \F^S$ is the function defined by
        \[ (\lambda f)(x) = \lambda f(x) \]
        for all $x \in S$.
        With these definitions, $\F^S$ becomes a vector space.
    \end{itemize}
\end{example}

Our first theorem then follows:

\begin{theorem}[A Number 0 Times a Vector]
    If $V$ is a vector space, $\forall u \in V$, $0u = 0$.
    \begin{proof*}
    For arbitrary $u \in V$, we have:
    \begin{align*}
        0u &= (0 + 0)u \\
        &= 0u + 0u
    \end{align*}
    Adding the additive inverse of $0u$, denoted $-0u$, to both sides of the equation
    above gives:
    \begin{align*}
        0u + (-0u) &= 0u + 0u + (-0u) \\
        0 &= 0u
    \end{align*}
    as desired. \qed
    \end{proof*}

\end{theorem}

Advantages of the abstract approach to vector spaces:
\begin{itemize}
    \item Can apply what was done in multiple new situations.
    \item Stripping away inessential properties leads to greater understanding.
\end{itemize}

If $V$ is a vector space, it would be incorrect to prove that $0u = 0$ for $u \in V$
by writing: Let $u = (x_1, \ldots, x_n)$, thus...

\begin{note}
    An element of $V$ is not necessarily of the form $(x_1, \ldots, x_n)$.
\end{note}

\subsection{Subspaces}

Let's add a new convention. From now on, $V$ denotes a vector space over $\F$
for brevity.

\begin{definition}[Subspace]
    A subset $U$ of $V$ is called a subspace of $V$ if $U$ is also a vector space
    (using the same addition and scalar multiplication as on $V$).
\end{definition}

\begin{example}
    $\{(x_1, x_2, 0) : x_1, x_2, \in \F \}$ is a subspace of $\F^3$
\end{example}

\begin{definition}[Conditions for a Subspace]
    A subset $U$ of $V$ is a subspace of $V$ if and only if $U$ satisfies the following
    three conditions:
    \begin{itemize}
        \item $0 \in U$
        \item $u, w \in U \implies u + w \in U$
        \item $\lambda \in \F, u \in U \implies \lambda u \in U$
    \end{itemize}
\end{definition}

Note that we do not need to check any of the other properties of a vector
space because we know that they will hold. Most of these properties are related
to the addition and multiplication properties, which we know hold since we're using
the same ones.

\begin{example} Examples of subspaces:
    \begin{itemize}
        \item If $b \in \F$, then
        \[ \{ (x_1, x_2, x_3, x_4) \in \F^4 : x_3 = 5x_4 + b \} \]
        is a subspace of $\F^4$ if and only if $b = 0$, in order to have
        the additive identity in the set.

        \item The set of continuous real-valued functions on the interval $[0,1]$
        is a subspace of $\R^{[0,1]}$. (The zero function is the identity in this case)

        \item The set of differentiable real-valued functions on $\R$ is a subspace 
        of $\R^{\R}$ (a sum of two differentiable functions is differentiable)

        \item The set of all sequences of complex numbers with limit 0 is a subspace
        of $\C^{\infty}$

        \item The subspaces of $\R^2$ are precisely $\{0\}, \R^2$ and all lines in $\R^2$
        through the origin.
    \end{itemize}
    
\end{example}

\begin{definition}[Sum of Subsets]
    Suppose $U_1, \ldots, U_m$ are subsets of $V$. The sum of $U_1, \ldots, U_m$, denoted
    $U_1 + \ldots + U_m$, is the set of all possible sums of elements of $U_1, \ldots, U_m$.
    \[ U_1 + \ldots + U_m = \{u_1 + \dots + u_m : u_1 \in U_1, \ldots, u_m \in U_m \} \]
\end{definition}

\begin{theorem}[Sum of Subspaces is the Smallest Containing Subspace]
    Suppose $U_1, \ldots, U_m$ are subspaces of $V$. Then $U_1 + \ldots U_m$ is the smallest
    subspace of $V$ containing $U_1, \ldots, U_m$.
\end{theorem}

\begin{definition}[Direct Sum]
    Suppose $U_1, \ldots, U_m$ are subspaces of $V$.

    The sum $U_1 + \ldots + U_m$ is called a direct sum if each element of $U_1 + \ldots + U_m$
    can be written in only one way as a sum $u_1 + \ldots + u_m$ where each $u_j$ is in $U_j$

    If the sum is indeed a direct sum, we use $\oplus$ between the symbols to denote that it is a direct sum.
\end{definition}

\begin{example}
    Suppose
    \[ U = \{(x, y, 0)\in \F^3 : x, y \in \F \}, W = \{(0, 0, z) \in \F^3 : z \in \F \} \]\
    Thus, $\F^3 = U \oplus W$.
\end{example}

These two theorems make it easy to see whether something is a direct sum.
\begin{theorem}[Condition for a Direct Sum]
    Suppose $U_1, \ldots, U_m$ are subspaces of $V$. Then $U_1 + \ldots + U_m$ is a 
    direct sum if and only if the only way to write 0 as a sum $u_1 + \ldots + u_m$, where
    each $u_j$ is in $U_j$, is by taking each $u_j$ equal to 0.
\end{theorem}

\begin{theorem}[Direct Sum of Two Subspaces]
   Suppose $U$ and $W$ are subspaces of $V$. Then $U + W$ is a direct sum
   if and only if $U \cap W = \{0\}$.
\end{theorem}
\endinput