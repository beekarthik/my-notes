\subsection{Lecture 7}

\subsubsection{Multiple Continuous Random Variables}

\begin{definition} [JCDF]
    For random variables $X$ and $Y$, the joint CDF (JCDF) is given by:
    \[ F_{X, Y}(x, y) = \Pr{X \leq x, Y \leq y} \] 
    For continuous random variables, this is:
    \[ F_{X,Y}(x, y) = \int_{\infty}^x \int_{\infty}^y f_{X, Y}(x', y') \dd{y'} \dd{x'} \]
\end{definition}

Then, note that to find $F_{\max{X, Y}}(k) = \Pr{\max{X, Y} \leq k} = \Pr{X \leq k, Y \leq k}$, we can invoke the JCDF (and make simplifications if $X$ and $Y$ are independent).
Furthermore, to find the probability density, we can simply differentiate with respect to $k$.

Similarly, $\Pr{\min{X, Y} > k} = \Pr{X > k, Y > k}$, which would be the JCCDF (which is the product of the CCDFs if $X$ and $Y$ are independent).

The sum of independent random variables is given by the convolution, *.
\begin{theorem} [Convolution]
    Let $Z = X + Y$ where $X$ and $Y$ are independent. Then,
    \[ f_Z(z) = f_X * f_Y = \int_{-\infty}^{\infty} f_X(x) f_Y(z - x) \dd{x} \]
\end{theorem}

In addition, we have conditioning for continuous random variables as well.
\begin{definition}[Conditional PDF]
    Consider two random variables, $X$ and $Y$ such that $f_X(x) \neq 0$ at the point we are considering. Then:
    \[ f_{X \mid Y}(x \mid y) = f_{XY}(x, y)/f_{Y}(y) \]
\end{definition}

\begin{definition}[Conditional Variance]
    Let $X$ and $Y$ be random variables. Then we define conditional variance as:
    \[ \Var{Y \mid X} = \E{Y^2 \mid X} - \qty(\E{Y \mid X})^2 \]
\end{definition}

Similar to the law of iterated expectation, there is a similar formula for variance:
\begin{theorem} [Law of Iterated Variance]
    Let $X$ and $Y$ be random variables. Then,
    \[ \Var{Y} = \E{\Var{Y \mid X}} + \Var{\E{Y \mid X}} \]

    \begin{proof*}
        \begin{align*}
            \Var{Y \mid X} &= \E{Y^2 \mid X} - \qty(\E{Y \mid X})^2 \\
            \E{\Var{Y \mid X}} &= \E{Y^2} - \E{\qty(\E{Y \mid X})^2} \\
            &= \Var{Y} + \qty(\E{Y})^2 -\E{\qty(\E{Y \mid X})^2} \\
            &= \Var{Y} + \qty(\E{\E{Y \mid X}})^2 - \E{\qty(\E{Y \mid X})^2} \\
            &= \Var{Y} - \Var{\E{Y \mid X}} \\
            \Var{Y} &= \E{\Var{Y \mid X}} + \Var{\E{Y \mid X}}
        \end{align*}
    \end{proof*}
\end{theorem}

An interpretation of this is to think about dividing $\Omega$ into disjoint sets where $S_i$ where $X = x_i$.
The conditional expectation, The first term is saying that taking the variance of replacing each set with the average over the set,
is the first-order
approximation. Then, to correct, we average the variances across each and add those in.

\section{Page Rank}

\subsection{Lecture 7, Continued}

Page rank is an algorithm originally used by Google for ranking the pages from a keyword search.

It tries to look at the problem as a Markov chain;
the weight of each page $p$ is sum over all pages linking to $p$, times
the probability of visiting $p$ from that other page. Symbolically, this is:
\[ \pi(i) = \sum_{j \in \mathcal{X}} \pi(j) P(j, i), \forall i \in \mathcal{X} \]
where $\pi$ is a (row) vector of the weights, and $P$ is a matrix whose $(j, i)$ entry
corresponds to the probability of transitioning from $j$ to $i$. So, we can rewrite the above equations
as
\[ \pi = \pi P \]
We also add the normalization constraint
\[ \sum_{i \in \mathcal{X}} \pi(i) = 1 \]
So, $\pi$ is a probability distribution.

\subsubsection{Discrete-Time Markov Chains}

\begin{definition}[Discrete Time Markov Chain (DTMC)]
    The DTMC $\{X(n), n \geq 0\}$ over state space $\mathcal{X}$ with $P = [P(i,j)]$ as the transition matrix
    with $P(i, j) = \Pr{X(n + 1) = j \mid X(n) = i}$.

    Markov chains have the memoryless property:
    \[\Pr{X(n + 1) = j \mid X(n) = i, X(m), m < n} = P(i, j) \forall i, j \]
\end{definition}

Generally, we have $P$ constant with respect to time, i.e. it's time-homogenous.
Note that we have not shown that defining probabilities like this is
consistent with the axioms of probability; let us assume such a choice exists for now.

Then, let $\pi_n(i)$ be the probability that the
Markov chain is in state $i$ at time $n$. Then, for every time step,
using vector notation, we get the recurrence:
\[ \pi_{n + 1} = \pi_n P \implies \pi_n = \pi_0 P^n \]

\begin{definition}[Stationary Distribution]
    Let $P$ be the transition matrix of a markov chain. We call $\pi$ a stationary distribution
    of the Markov chain if:
    \[\pi = \pi P \text{ and } \sum_{i = 1}^n \pi(i) = 1\]
    We call these equations the balance equations.
\end{definition}

One thing to note is that we can rewrite the equations as $(P - I) \pi = 0$. However,
since every row sums to 0, this is not a full rank matrix, so we need the extra constraint about
the sum of all the elements of $\pi$.

Next, we have some ways to classify Markov chains.

\begin{definition}[Irreducible]
    A Markov Chain is irreducible if it can reach any state from any other state (possibly in multiple steps).
\end{definition}

\begin{definition}[Aperiodic]
    Let $d(i) = \gcd\{n \geq 1 \mid P^n(i, i) > 0\}$. An irreducible DTMC is aperiodic if $d(i) = 1$ for all $i$
    (in fact in an irreducible DTMC, $d(i)$ is the same as $i$).
\end{definition}

Note that if a Markov chain has a self loop, it is aperiodic because we can
get from $i$ to $i$ in a single step, so $d(i) = 1$ always.

\begin{example}
    Consider the following Markov chain, which is irreducible and aperiodic.
    (DIAGRAM NEEDED)
    \[ P = \begin{bmatrix}
        0 & 1 & 0 \\
        0.6 & 0 & 0.4 \\
        0 & 0.9 & 0.1
    \end{bmatrix} \]
    Note that $P(i, j)$ is the probability of going from $i$ to $j$. Note that all the rows sum to
    1 (by the total probability rule). Now let us solve the balance equations:
    \begin{align*}
        \pi &= \pi P \\
        \begin{bmatrix}
            \pi_0 & \pi_1 & \pi_2
        \end{bmatrix} &= \begin{bmatrix}
            \pi_0 & \pi_1 & \pi_2
        \end{bmatrix} \begin{bmatrix}
            0 & 1 & 0 \\
            0.6 & 0 & 0.4 \\
            0 & 0.9 & 0.1
        \end{bmatrix} \\
    \end{align*}
    This yields the equations:
    \[
        \begin{cases}
            0.6 \pi_1 = \pi_0 \\
            \pi_0 + 0.9 \pi_2 = \pi_1 \\
            0.4 \pi_1 + 0.1 \pi_2 = \pi_2 \\
            \pi_0 + \pi_1 + \pi_2 = 0
        \end{cases}
    \]
    Note that some of these equations are dependent, so we needed the extra constraint.
    This can then be solved to get:
    \[ \pi = \begin{bmatrix} 0.294 & 0.489 & 0.217 \end{bmatrix} \]
\end{example}

With these notions, we have the following results.

\begin{theorem} [Big Theorem for Finite DTMC]
    Consider an irreducible DTMC over a finite state space. Then,
    \begin{itemize}
        \item There is a unique invariant distribution $\pi$
        \item The long-term fraction of time spent in state $n$ is given by
        \[ \lim_{N \to \infty} \frac{1}{N} \sum_{n = 0}^{N - 1} \mathbbm{1}_{\{X(n) = i\}} = \pi(i) \]
        \item If the DTMC is aperiodic, $\pi_n \to \pi$ as $n \to \infty$, independent of $\pi_0$
    \end{itemize}
\end{theorem}

Another way to state the last result is that each row in $P^n$ converges to $\pi$ as $n \to \infty$. Why?
Consider $\pi_0 = \begin{bmatrix}
    1 & 0 & \dots & 0
\end{bmatrix}$. Then $\pi_n = \pi_0 P^n = p_1 \to \pi$, so the first row approaches $\pi$.
The argument is similar for any row.