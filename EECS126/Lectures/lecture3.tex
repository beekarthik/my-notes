\subsection{Lecture 3}

\begin{theorem} [Markov's Inequality]
    Consider random variable $X \geq 0$ and constant $a > 0$. Then,
    \[ \Pr{X \geq a} \leq \frac{\E{X}}{a} \]
\end{theorem}

Here is a quick proof of this fact. Note that $\mathbbm{1}_A$
is the indicator for event $A$, i.e. a random variable with the following values:

\[ \mathbbm{1}_A = 
\begin{cases}
   1 & \text{ if sample point in event $A$} \\
   0 & \text{ otherwise}
\end{cases}
\]

Let $Y = \mathbbm{1}_{X \geq a}$. Then we know:

\begin{align*}
    \E{Y} &= 0 \cdot \Pr{X < a} + 1 \cdot \Pr{X \geq a} = \Pr{X \geq a} \\
    Y &\leq \frac{X}{a} \\
    \E{Y} &\leq \E{\frac{X}{a}} = \frac{\E{X}}{a} \\
    \Pr{X \geq a} &\leq \frac{\E{X}}{a}
\end{align*}

Markov's inequality tends to be a coarse bound, and $X, a$ have to be non-negative.

\begin{theorem} [Chebyshev's Inequality]
    For random variable $X$ and $\epsilon > 0$:
    \[ \Pr{\abs{X - \E{X}}} \geq \epsilon) \leq \frac{\Var{X}}{\epsilon^2} \]
\end{theorem}

Define $Z = \abs{X - \E{X}}^2$, $a = \epsilon^2$, $\epsilon > 0$.

Apply Markov's inequality:
\[ \Pr{Z \geq \epsilon^2} \leq \frac{\E{Z}}{\epsilon^2} \]

Note that \[\E{Z} = \E{(X - \E{X})^2} = \Var{X} \]

This means that \[ \Pr{\sqrt{Z} \geq \epsilon} \leq \frac{\Var{X}}{\epsilon^2} \]
which is exactly the statement of Chebyshev's.

Chebyshev's is generally a tighter bound than Markov's.

\begin{theorem}[Weak Law of Large Numbers]
    Assume $X_1, X_2, X_3, \dots$ are independent random variables with the same expectation
    $\mu$ and the same variance $\sigma^2$, and define $Y_n = \frac{(X_1 + X_2 + \dots + X_n)}{n}$
    Then, we have that for any constant $\epsilon > 0$.
    \[ \lim_{n \to \infty} \Pr{\abs{Y_n - \mu} \geq \epsilon} \to 0 \]
\end{theorem}

This can be shown by Chebyshev's inequality, namely note that the expression in the limit
is bounded by $\frac{\Var{Y_n}}{\epsilon^2} = \frac{n\sigma^2}{n^2\epsilon^2} \to 0$.

In words, this means the probability of the sample mean being within $\epsilon$ of the true mean
approaches 1.

\begin{definition} [Covariance]
    \[ \Cov{X, Y} = \E{XY} - \E{X}\E{Y} = \E{(X - \E{X})(Y - \E{Y})} \]
\end{definition}

If $X$ and $Y$ are independent, then $\Cov{X, Y} = 0$. If the latter is true,
then $X$ and $Y$ are uncorrelated. We also define the coefficient of correlation:

\[ \rho_{xy} = \frac{\Cov{X, Y}}{\sigma_X \sigma_Y} \]

This is handy because $\abs{\rho_{XY}} \leq 1$.

Suppose we want to estimate a random variable $Y$ by $\hat{Y}$ given a correlated
random variable $Y$.

We want to minimize $\E{\qty(Y - \hat{Y})^2}$ but have a linear relationship.
This yields LLSE:

\begin{theorem} [Linear Least Squares Estimate]
    The LLSE
    \[ \hat{Y} = \E{Y} + \frac{\Cov{X, Y}}{\Var{X}}\qty(X - \E{X}) \]
    is the best linear estimate of $Y$ given $X$.
\end{theorem}

\section{Basic Probability}

\subsection{Lecture 3, Continued}

There are some important consequences of the axioms of probability.

\begin{theorem} [Convergence]
    Consider some set $A$ where $A = \bigcup_{n = 1}^{\infty} A_n$ where:
    \[ A_1 \subseteq A_2 \subseteq \dots \]

    Then, $\Pr{A_n} \rightarrow \Pr{A}$.

    Furthermore, consider some set $B$ where $B = \bigcap_{n = 1}^{\infty} B_n$ where:
    \[ B_1 \supseteq B_2 \supseteq \dots \]
    Then, $\Pr{B_n} \rightarrow \Pr{A}$.
\end{theorem}

\begin{theorem} [Borel-Cantelli Theorem]
    Let $\{A_n\}_{n = 1}^{\infty}$ be a collection of events such that $\sum_{n = 1}^{\infty} \Pr{A_n} < \infty$, then
    $\Pr{A_n \text{ infinitely often}} = 0$.

    $\{A_n \text{ infinitely often}\}$ is the complement of the following event:
    \[ \{\omega \mid \exists N(\omega) \text{ such that } \forall n > N(\omega), \omega \not\in A_n \} \]

    In English, the set describes all outcomes where you can assign a number to that outcome such that
    after $A_N$, you have no set membership.

    The theorem claims that you CANNOT assign such a number to any outcome with nonzero probability.
\end{theorem}

\begin{example} [Infinite Flips]
    \begin{align*}
        \{A_n \text{ i o } \} = \{ \omega | \not\exists N(\omega) \} \\
    \end{align*}
\end{example}